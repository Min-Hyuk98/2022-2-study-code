{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### augmentation은 동일\n",
    "### 현재 학습한거 CE + 현재 학습한거와 이전까지EMA MSE 한건데...\n",
    "### 그냥 momentum 계열의 optimizer 쓴거와 뭐가 다른건지..\n",
    "### 앞에서 augmentation이라도 따로 한것도 아니고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(root, transform):\n",
    "    # load train data\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=True)\n",
    "\n",
    "    # load test data\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=root,\n",
    "        train=False,\n",
    "        transform=transform, download=True)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_train(train_dataset, test_dataset, batch_size, k, n_classes, seed, shuffle_train=False, return_idx=True):\n",
    "    '''Randomly form unlabeled data in training dataset'''\n",
    "\n",
    "    n = len(train_dataset)  # dataset size\n",
    "    rrng = np.random.RandomState(seed) # seed \n",
    "    indices = torch.zeros(k)  # indices of keep labeled data\n",
    "    others = torch.zeros(n - k)  # indices of unlabeled data\n",
    "    card = k // n_classes\n",
    "    cpt = 0\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        class_items = (train_dataset.train_labels == i).nonzero()  # indices of samples with label i\n",
    "        n_class = len(class_items)  # number of samples with label i\n",
    "        rd = rrng.permutation(np.arange(n_class))  # shuffle them\n",
    "        indices[i * card: (i+1) * card] = torch.squeeze(class_items[rd[:card]])\n",
    "        others[cpt: cpt+n_class-card] = torch.squeeze(class_items[rd[card:]])\n",
    "        cpt += (n_class-card)\n",
    "\n",
    "    # tensor as indices must be long, byte or bool\n",
    "    others = others.long()\n",
    "    train_dataset.train_labels[others] = -1\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               num_workers=2,\n",
    "                                               shuffle=shuffle_train)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=2,\n",
    "                                              shuffle=False)\n",
    "\n",
    "    if return_idx:\n",
    "        return train_loader, test_loader, indices\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    # \"\"\"데이터에 noise 추가\"\"\"\n",
    "    def __init__(self, batch_size, input_shape, std):\n",
    "        super(GaussianNoise, self).__init__()\n",
    "        self.shape = (batch_size, ) + input_shape\n",
    "        self.std = std\n",
    "        self.noise = torch.zeros(self.shape).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.noise.normal_(mean=0, std=self.std)\n",
    "        # print(self.noise.shape)\n",
    "\n",
    "        return x + self.noise\n",
    "\n",
    "def temporal_losses(out1, out2, w, labels):\n",
    "    # output1: current output\n",
    "    # output2: temporal output\n",
    "    # w: weight for summation loss\n",
    "\n",
    "    # \"ensemble output과 current output을 통해 supervised, unsupervised loss 및 total loss를 계산함\"\n",
    "\n",
    "    sup_loss, nbsup = masked_crossentropy(out1, labels)\n",
    "    unsup_loss = mse_loss(out1, out2)\n",
    "    total_loss = sup_loss + w * unsup_loss\n",
    "\n",
    "    return total_loss, sup_loss, unsup_loss, nbsup\n",
    "\n",
    "def mse_loss(out1, out2):\n",
    "    # \"current output, ensemble output 간의 mean difference: unsupervised loss\"\n",
    "    quad_diff = torch.sum((F.softmax(out1, dim=1) - F.softmax(out2, dim=1)) ** 2)\n",
    "\n",
    "    return quad_diff / out1.data.nelement()\n",
    "\n",
    "def masked_crossentropy(out, labels):\n",
    "    # \"labeld된 data에 한해서 crossentropy loss를 계산함\"\n",
    "    cond = (labels >= 0)\n",
    "    nnz = torch.nonzero(cond)  # array of labeled sample index\n",
    "    nbsup = len(nnz)  # number of supervised samples\n",
    "    # check if labeled samples in batch, return 0 if none\n",
    "    if nbsup > 0:\n",
    "        # select lines in out with label\n",
    "        masked_outputs = torch.index_select(out, 0, nnz.view(nbsup))\n",
    "        masked_labels = labels[cond]\n",
    "        loss = F.cross_entropy(masked_outputs, masked_labels)\n",
    "        return loss, nbsup\n",
    "    loss = torch.tensor([0.], requires_grad=False).cuda()\n",
    "    return loss, 0\n",
    "\n",
    "def weight_scheduler(epoch, max_epochs, max_val, mult, n_labeled, n_samples):\n",
    "    \"epoch이 지남에 따라 weight를 조정함\"\n",
    "    max_val = max_val * (float(n_labeled) / n_samples)\n",
    "    return ramp_up(epoch, max_epochs, max_val, mult)\n",
    "\n",
    "def ramp_up(epoch, max_epochs, max_val, mult):\n",
    "    # \"weight를 조정하며 첫 epoch에는 0을 사용함\"\n",
    "    if epoch == 0:\n",
    "        return 0.\n",
    "    elif epoch >= max_epochs:\n",
    "        return max_val\n",
    "    return max_val * np.exp(-mult * (1. - float(epoch) / max_epochs) ** 2)\n",
    "\n",
    "def calc_metrics(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (samples, labels) in enumerate(loader):\n",
    "        samples = samples.cuda()\n",
    "        labels = labels.requires_grad_(False).cuda()\n",
    "        outputs = model(samples)\n",
    "        _, predicted = torch.max(outputs.detach(), 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.detach().view_as(predicted)).sum()\n",
    "    acc = 100 * float(correct) / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, batch_size, std, input_shape=(1, 28, 28), p=0.5, fm1=16, fm2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        self.std = std\n",
    "        self.p = p\n",
    "        self.fm1 = fm1\n",
    "        self.fm2 = fm2\n",
    "        self.input_shape = input_shape\n",
    "        self.conv_block1 = nn.Sequential(nn.Conv2d(1, self.fm1, 3, stride=1, padding=1),\n",
    "                                        nn.BatchNorm2d(self.fm1), \n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(3, stride=2, padding=1)\n",
    "                                      )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(nn.Conv2d(self.fm1, self.fm2, 3, stride=1, padding=1),\n",
    "                                        nn.BatchNorm2d(self.fm2), \n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(3, stride=2, padding=1)\n",
    "                                      )\n",
    "        self.drop = nn.Dropout(self.p)\n",
    "        self.fc = nn.Linear(self.fm2 * 7 * 7, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            b = x.size(0)\n",
    "            gn = GaussianNoise(b, self.input_shape, self.std)\n",
    "            x = gn(x)\n",
    "\n",
    "        # first block\n",
    "        x = self.conv_block1(x)\n",
    "        \n",
    "        # second block\n",
    "        x = self.conv_block2(x)\n",
    "\n",
    "        # classifier\n",
    "        x = x.view(-1, self.fm2 * 7 * 7)\n",
    "        x = self.fc(self.drop(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader ,seed, k, alpha, lr, num_epochs, batch_size, ntrain,n_classes=10, max_epochs=80, max_val=1.):\n",
    "\n",
    "    # build model and feed to GPU\n",
    "    model.cuda()\n",
    "\n",
    "    # setup param optimization\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99))\n",
    "\n",
    "    # model.train()\n",
    "    \n",
    "    # 첫 ensemble ouput은 모두 0\n",
    "    # 한 에폭 내에서 각 minibatch iter마다 outputs에 차곡차곡 쌓음\n",
    "    # 각 minibatch iter마다 z에 저장된 값 이용해서 loss에 반영함\n",
    "    # z는 한 에폭이 끝난 후 현재 에폭의 정보인 outputs와 이전 정보(z) EMA해서 갱신\n",
    "    Z = torch.zeros(ntrain, n_classes).float().cuda()  # intermediate values\n",
    "    z = torch.zeros(ntrain, n_classes).float().cuda()  # temporal outputs\n",
    "    outputs = torch.zeros(ntrain, n_classes).float().cuda()  # current outputs\n",
    "\n",
    "    losses = []\n",
    "    suplosses = []\n",
    "    unsuplosses = []\n",
    "    best_loss = 30.0\n",
    "    for epoch in range(num_epochs):\n",
    "        t = timer()\n",
    "        print('\\nEpoch: {}'.format(epoch+1))\n",
    "        model.train()\n",
    "        # evaluate unsupervised cost weight\n",
    "        w = weight_scheduler(epoch, max_epochs, max_val, 5, k, 60000)\n",
    "\n",
    "        w = torch.tensor(w, requires_grad=False).cuda()\n",
    "        print('---------------------')\n",
    "\n",
    "        # targets change only once per epoch\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            #print(i)\n",
    "            batch_size = images.size(0)  # retrieve batch size again cause drop last is false\n",
    "            images = images.cuda()\n",
    "            labels = labels.requires_grad_(False).cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(images)\n",
    "            # 현재 batch에 맞는 ensemble 결과들을 가져옴\n",
    "            zcomp = z[i * batch_size: (i+1) * batch_size]\n",
    "            zcomp.requires_grad_(False)\n",
    "            loss, suploss, unsuploss, nbsup = temporal_losses(out, zcomp, w, labels)\n",
    "\n",
    "            # save outputs\n",
    "            outputs[i * batch_size: (i+1) * batch_size] = out.clone().detach()\n",
    "            losses.append(loss.item())\n",
    "            suplosses.append(nbsup * suploss.item())\n",
    "            unsuplosses.append(unsuploss.item())\n",
    "\n",
    "            # backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_mean = np.mean(losses)\n",
    "        supl_mean = np.mean(suplosses)\n",
    "        unsupl_mean = np.mean(unsuplosses)\n",
    "\n",
    "        print('Epoch [%d/%d], Loss: %.6f, Supervised Loss: %.6f, Unsupervised Loss: %.6f, Time: %.2f' %\n",
    "              (epoch + 1, num_epochs, float(loss_mean), float(supl_mean), float(unsupl_mean), timer()-t))\n",
    "        # model의 outputs을 가중평균을 이용해 ensemble outputs으로 update 함\n",
    "        Z = alpha * Z + (1. - alpha) * outputs\n",
    "        z = Z * (1. / (1. - alpha ** (epoch + 1)))\n",
    "\n",
    "        if loss_mean < best_loss:\n",
    "            best_loss = loss_mean\n",
    "            torch.save({'state_dict': model.state_dict()}, 'model_best.pth')\n",
    "\n",
    "        model.eval()\n",
    "        acc = calc_metrics(model, val_loader)\n",
    "        print('Acc : %.2f' % acc)\n",
    "\n",
    "def evaluation(model, loader):\n",
    "\n",
    "    # test best model\n",
    "    checkpoint = torch.load('model_best.pth')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (samples, labels) in enumerate(loader):\n",
    "        samples = samples.cuda()\n",
    "        labels = labels.requires_grad_(False).cuda()\n",
    "        outputs = model(samples)\n",
    "        _, predicted = torch.max(outputs.detach(), 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.detach().view_as(predicted)).sum()\n",
    "    acc = 100 * float(correct) / total\n",
    "    print('Acc (best model): %.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        # global vars\n",
    "        self.n_exp = 5 # number of experiments, try 5 different seed\n",
    "        self.k = 100 # keep k labeled data in whole training set, other without label\n",
    "\n",
    "        # dataset vars\n",
    "        self.m = 0.1307\n",
    "        self.s = 0.3081\n",
    "\n",
    "        # model vars\n",
    "        self.drop = 0.5 # dropout probability\n",
    "        self.std = 0.15 # std of gaussian noise\n",
    "        self.fm1 = 32 # channels of the first conv\n",
    "        self.fm2 = 64 # channels of the second conv\n",
    "        self.w_norm = True\n",
    "\n",
    "        # optim vars\n",
    "        self.learning_rate = 0.002\n",
    "        self.beta2 = 0.99 # second momentum for Adam\n",
    "        self.num_epochs = 50\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # temporal ensembling vars\n",
    "        self.alpha = 0.6 # ensembling momentum\n",
    "        self.data_norm = 'channelwise' # image normalization\n",
    "        self.divide_by_bs = False # whether we divide supervised cost by batch_size\n",
    "\n",
    "        # RNG\n",
    "        self.rng = np.random.RandomState(42)\n",
    "        self.seeds = [self.rng.randint(200) for _ in range(self.n_exp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "---------------------\n",
      "Epoch [1/50], Loss: 0.765479, Supervised Loss: 0.809218, Unsupervised Loss: 0.065700, Time: 6.61\n",
      "Acc : 31.85\n",
      "\n",
      "Epoch: 2\n",
      "---------------------\n",
      "Epoch [2/50], Loss: 0.552805, Supervised Loss: 0.579083, Unsupervised Loss: 0.061820, Time: 6.38\n",
      "Acc : 42.54\n",
      "\n",
      "Epoch: 3\n",
      "---------------------\n",
      "Epoch [3/50], Loss: 0.421480, Supervised Loss: 0.441230, Unsupervised Loss: 0.061802, Time: 6.73\n",
      "Acc : 70.33\n",
      "\n",
      "Epoch: 4\n",
      "---------------------\n",
      "Epoch [4/50], Loss: 0.347495, Supervised Loss: 0.363312, Unsupervised Loss: 0.059535, Time: 6.45\n",
      "Acc : 60.76\n",
      "\n",
      "Epoch: 5\n",
      "---------------------\n",
      "Epoch [5/50], Loss: 0.291636, Supervised Loss: 0.304673, Unsupervised Loss: 0.055390, Time: 6.62\n",
      "Acc : 71.49\n",
      "\n",
      "Epoch: 6\n",
      "---------------------\n",
      "Epoch [6/50], Loss: 0.253021, Supervised Loss: 0.263902, Unsupervised Loss: 0.051658, Time: 6.88\n",
      "Acc : 79.96\n",
      "\n",
      "Epoch: 7\n",
      "---------------------\n",
      "Epoch [7/50], Loss: 0.222358, Supervised Loss: 0.231762, Unsupervised Loss: 0.048294, Time: 7.03\n",
      "Acc : 80.06\n",
      "\n",
      "Epoch: 8\n",
      "---------------------\n",
      "Epoch [8/50], Loss: 0.199676, Supervised Loss: 0.207910, Unsupervised Loss: 0.045544, Time: 7.07\n",
      "Acc : 72.62\n",
      "\n",
      "Epoch: 9\n",
      "---------------------\n",
      "Epoch [9/50], Loss: 0.182616, Supervised Loss: 0.190008, Unsupervised Loss: 0.043243, Time: 6.95\n",
      "Acc : 77.54\n",
      "\n",
      "Epoch: 10\n",
      "---------------------\n",
      "Epoch [10/50], Loss: 0.166963, Supervised Loss: 0.173793, Unsupervised Loss: 0.041356, Time: 7.11\n",
      "Acc : 85.75\n",
      "\n",
      "Epoch: 11\n",
      "---------------------\n",
      "Epoch [11/50], Loss: 0.153510, Supervised Loss: 0.159720, Unsupervised Loss: 0.039505, Time: 6.91\n",
      "Acc : 85.41\n",
      "\n",
      "Epoch: 12\n",
      "---------------------\n",
      "Epoch [12/50], Loss: 0.142071, Supervised Loss: 0.147764, Unsupervised Loss: 0.038072, Time: 7.00\n",
      "Acc : 87.36\n",
      "\n",
      "Epoch: 13\n",
      "---------------------\n",
      "Epoch [13/50], Loss: 0.131988, Supervised Loss: 0.137289, Unsupervised Loss: 0.036942, Time: 6.97\n",
      "Acc : 85.08\n",
      "\n",
      "Epoch: 14\n",
      "---------------------\n",
      "Epoch [14/50], Loss: 0.123364, Supervised Loss: 0.128287, Unsupervised Loss: 0.035740, Time: 7.02\n",
      "Acc : 87.77\n",
      "\n",
      "Epoch: 15\n",
      "---------------------\n",
      "Epoch [15/50], Loss: 0.115446, Supervised Loss: 0.120041, Unsupervised Loss: 0.034626, Time: 7.01\n",
      "Acc : 84.69\n",
      "\n",
      "Epoch: 16\n",
      "---------------------\n",
      "Epoch [16/50], Loss: 0.108522, Supervised Loss: 0.112830, Unsupervised Loss: 0.033553, Time: 6.87\n",
      "Acc : 87.52\n",
      "\n",
      "Epoch: 17\n",
      "---------------------\n",
      "Epoch [17/50], Loss: 0.102200, Supervised Loss: 0.106255, Unsupervised Loss: 0.032530, Time: 7.21\n",
      "Acc : 89.02\n",
      "\n",
      "Epoch: 18\n",
      "---------------------\n",
      "Epoch [18/50], Loss: 0.096550, Supervised Loss: 0.100379, Unsupervised Loss: 0.031606, Time: 7.15\n",
      "Acc : 90.74\n",
      "\n",
      "Epoch: 19\n",
      "---------------------\n",
      "Epoch [19/50], Loss: 0.091754, Supervised Loss: 0.095381, Unsupervised Loss: 0.030705, Time: 7.11\n",
      "Acc : 89.90\n",
      "\n",
      "Epoch: 20\n",
      "---------------------\n",
      "Epoch [20/50], Loss: 0.087204, Supervised Loss: 0.090650, Unsupervised Loss: 0.029811, Time: 7.09\n",
      "Acc : 91.13\n",
      "\n",
      "Epoch: 21\n",
      "---------------------\n",
      "Epoch [21/50], Loss: 0.083052, Supervised Loss: 0.086334, Unsupervised Loss: 0.028892, Time: 7.05\n",
      "Acc : 92.18\n",
      "\n",
      "Epoch: 22\n",
      "---------------------\n",
      "Epoch [22/50], Loss: 0.079447, Supervised Loss: 0.082580, Unsupervised Loss: 0.028091, Time: 7.42\n",
      "Acc : 92.42\n",
      "\n",
      "Epoch: 23\n",
      "---------------------\n",
      "Epoch [23/50], Loss: 0.076003, Supervised Loss: 0.079000, Unsupervised Loss: 0.027351, Time: 7.16\n",
      "Acc : 92.62\n",
      "\n",
      "Epoch: 24\n",
      "---------------------\n",
      "Epoch [24/50], Loss: 0.072897, Supervised Loss: 0.075769, Unsupervised Loss: 0.026647, Time: 7.12\n",
      "Acc : 92.60\n",
      "\n",
      "Epoch: 25\n",
      "---------------------\n",
      "Epoch [25/50], Loss: 0.069982, Supervised Loss: 0.072738, Unsupervised Loss: 0.025962, Time: 7.22\n",
      "Acc : 92.90\n",
      "\n",
      "Epoch: 26\n",
      "---------------------\n",
      "Epoch [26/50], Loss: 0.067290, Supervised Loss: 0.069941, Unsupervised Loss: 0.025286, Time: 7.05\n",
      "Acc : 93.64\n",
      "\n",
      "Epoch: 27\n",
      "---------------------\n",
      "Epoch [27/50], Loss: 0.064798, Supervised Loss: 0.067351, Unsupervised Loss: 0.024694, Time: 7.17\n",
      "Acc : 93.58\n",
      "\n",
      "Epoch: 28\n",
      "---------------------\n",
      "Epoch [28/50], Loss: 0.062505, Supervised Loss: 0.064966, Unsupervised Loss: 0.024125, Time: 7.23\n",
      "Acc : 93.94\n",
      "\n",
      "Epoch: 29\n",
      "---------------------\n",
      "Epoch [29/50], Loss: 0.060612, Supervised Loss: 0.062988, Unsupervised Loss: 0.023578, Time: 7.00\n",
      "Acc : 93.92\n",
      "\n",
      "Epoch: 30\n",
      "---------------------\n",
      "Epoch [30/50], Loss: 0.058596, Supervised Loss: 0.060893, Unsupervised Loss: 0.023075, Time: 7.10\n",
      "Acc : 93.99\n",
      "\n",
      "Epoch: 31\n",
      "---------------------\n",
      "Epoch [31/50], Loss: 0.056706, Supervised Loss: 0.058928, Unsupervised Loss: 0.022587, Time: 6.97\n",
      "Acc : 94.09\n",
      "\n",
      "Epoch: 32\n",
      "---------------------\n",
      "Epoch [32/50], Loss: 0.055814, Supervised Loss: 0.057967, Unsupervised Loss: 0.022127, Time: 7.19\n",
      "Acc : 94.22\n",
      "\n",
      "Epoch: 33\n",
      "---------------------\n",
      "Epoch [33/50], Loss: 0.054122, Supervised Loss: 0.056211, Unsupervised Loss: 0.021686, Time: 7.21\n",
      "Acc : 94.37\n",
      "\n",
      "Epoch: 34\n",
      "---------------------\n",
      "Epoch [34/50], Loss: 0.052531, Supervised Loss: 0.054557, Unsupervised Loss: 0.021250, Time: 7.17\n",
      "Acc : 94.81\n",
      "\n",
      "Epoch: 35\n",
      "---------------------\n",
      "Epoch [35/50], Loss: 0.051124, Supervised Loss: 0.053093, Unsupervised Loss: 0.020844, Time: 7.29\n",
      "Acc : 94.98\n",
      "\n",
      "Epoch: 36\n",
      "---------------------\n",
      "Epoch [36/50], Loss: 0.049706, Supervised Loss: 0.051620, Unsupervised Loss: 0.020459, Time: 7.43\n",
      "Acc : 95.14\n",
      "\n",
      "Epoch: 37\n",
      "---------------------\n",
      "Epoch [37/50], Loss: 0.048620, Supervised Loss: 0.050483, Unsupervised Loss: 0.020089, Time: 7.36\n",
      "Acc : 95.26\n",
      "\n",
      "Epoch: 38\n",
      "---------------------\n",
      "Epoch [38/50], Loss: 0.047341, Supervised Loss: 0.049154, Unsupervised Loss: 0.019728, Time: 7.32\n",
      "Acc : 95.24\n",
      "\n",
      "Epoch: 39\n",
      "---------------------\n",
      "Epoch [39/50], Loss: 0.046274, Supervised Loss: 0.048040, Unsupervised Loss: 0.019389, Time: 7.38\n",
      "Acc : 94.51\n",
      "\n",
      "Epoch: 40\n",
      "---------------------\n",
      "Epoch [40/50], Loss: 0.045542, Supervised Loss: 0.047265, Unsupervised Loss: 0.019081, Time: 7.00\n",
      "Acc : 95.33\n",
      "\n",
      "Epoch: 41\n",
      "---------------------\n",
      "Epoch [41/50], Loss: 0.044444, Supervised Loss: 0.046125, Unsupervised Loss: 0.018770, Time: 7.02\n",
      "Acc : 95.60\n",
      "\n",
      "Epoch: 42\n",
      "---------------------\n",
      "Epoch [42/50], Loss: 0.043393, Supervised Loss: 0.045033, Unsupervised Loss: 0.018481, Time: 7.38\n",
      "Acc : 95.36\n",
      "\n",
      "Epoch: 43\n",
      "---------------------\n",
      "Epoch [43/50], Loss: 0.042386, Supervised Loss: 0.043987, Unsupervised Loss: 0.018211, Time: 7.38\n",
      "Acc : 95.81\n",
      "\n",
      "Epoch: 44\n",
      "---------------------\n",
      "Epoch [44/50], Loss: 0.041426, Supervised Loss: 0.042992, Unsupervised Loss: 0.017937, Time: 7.44\n",
      "Acc : 95.42\n",
      "\n",
      "Epoch: 45\n",
      "---------------------\n",
      "Epoch [45/50], Loss: 0.040609, Supervised Loss: 0.042140, Unsupervised Loss: 0.017678, Time: 7.66\n",
      "Acc : 95.61\n",
      "\n",
      "Epoch: 46\n",
      "---------------------\n",
      "Epoch [46/50], Loss: 0.039730, Supervised Loss: 0.041227, Unsupervised Loss: 0.017423, Time: 7.17\n",
      "Acc : 95.80\n",
      "\n",
      "Epoch: 47\n",
      "---------------------\n",
      "Epoch [47/50], Loss: 0.038884, Supervised Loss: 0.040350, Unsupervised Loss: 0.017171, Time: 7.14\n",
      "Acc : 96.46\n",
      "\n",
      "Epoch: 48\n",
      "---------------------\n",
      "Epoch [48/50], Loss: 0.038083, Supervised Loss: 0.039517, Unsupervised Loss: 0.016935, Time: 7.01\n",
      "Acc : 96.16\n",
      "\n",
      "Epoch: 49\n",
      "---------------------\n",
      "Epoch [49/50], Loss: 0.037520, Supervised Loss: 0.038925, Unsupervised Loss: 0.016708, Time: 7.23\n",
      "Acc : 95.92\n",
      "\n",
      "Epoch: 50\n",
      "---------------------\n",
      "Epoch [50/50], Loss: 0.036770, Supervised Loss: 0.038147, Unsupervised Loss: 0.016481, Time: 6.97\n",
      "Acc : 96.45\n",
      "Acc (best model): 96.45\n",
      "\n",
      "Epoch: 1\n",
      "---------------------\n",
      "Epoch [1/50], Loss: 0.781874, Supervised Loss: 0.818476, Unsupervised Loss: 0.066684, Time: 7.13\n",
      "Acc : 27.91\n",
      "\n",
      "Epoch: 2\n",
      "---------------------\n",
      "Epoch [2/50], Loss: 0.545780, Supervised Loss: 0.569137, Unsupervised Loss: 0.063167, Time: 7.30\n",
      "Acc : 54.08\n",
      "\n",
      "Epoch: 3\n",
      "---------------------\n",
      "Epoch [3/50], Loss: 0.410585, Supervised Loss: 0.426763, Unsupervised Loss: 0.059421, Time: 7.27\n",
      "Acc : 61.03\n",
      "\n",
      "Epoch: 4\n",
      "---------------------\n",
      "Epoch [4/50], Loss: 0.331610, Supervised Loss: 0.344149, Unsupervised Loss: 0.057154, Time: 7.23\n",
      "Acc : 70.34\n",
      "\n",
      "Epoch: 5\n",
      "---------------------\n",
      "Epoch [5/50], Loss: 0.279219, Supervised Loss: 0.289769, Unsupervised Loss: 0.053665, Time: 7.20\n",
      "Acc : 79.61\n",
      "\n",
      "Epoch: 6\n",
      "---------------------\n",
      "Epoch [6/50], Loss: 0.245376, Supervised Loss: 0.254231, Unsupervised Loss: 0.049158, Time: 6.94\n",
      "Acc : 78.37\n",
      "\n",
      "Epoch: 7\n",
      "---------------------\n",
      "Epoch [7/50], Loss: 0.214539, Supervised Loss: 0.222138, Unsupervised Loss: 0.045464, Time: 7.06\n",
      "Acc : 79.92\n",
      "\n",
      "Epoch: 8\n",
      "---------------------\n",
      "Epoch [8/50], Loss: 0.191856, Supervised Loss: 0.198565, Unsupervised Loss: 0.042240, Time: 7.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 80.15\n",
      "\n",
      "Epoch: 9\n",
      "---------------------\n",
      "Epoch [9/50], Loss: 0.173898, Supervised Loss: 0.179889, Unsupervised Loss: 0.039826, Time: 7.36\n",
      "Acc : 77.87\n",
      "\n",
      "Epoch: 10\n",
      "---------------------\n",
      "Epoch [10/50], Loss: 0.159780, Supervised Loss: 0.165246, Unsupervised Loss: 0.038018, Time: 7.18\n",
      "Acc : 81.50\n",
      "\n",
      "Epoch: 11\n",
      "---------------------\n",
      "Epoch [11/50], Loss: 0.146967, Supervised Loss: 0.151937, Unsupervised Loss: 0.036467, Time: 7.21\n",
      "Acc : 78.86\n",
      "\n",
      "Epoch: 12\n",
      "---------------------\n",
      "Epoch [12/50], Loss: 0.136103, Supervised Loss: 0.140689, Unsupervised Loss: 0.035116, Time: 7.10\n",
      "Acc : 83.58\n",
      "\n",
      "Epoch: 13\n",
      "---------------------\n",
      "Epoch [13/50], Loss: 0.125796, Supervised Loss: 0.130030, Unsupervised Loss: 0.033915, Time: 7.08\n",
      "Acc : 86.66\n",
      "\n",
      "Epoch: 14\n",
      "---------------------\n",
      "Epoch [14/50], Loss: 0.118158, Supervised Loss: 0.122089, Unsupervised Loss: 0.032863, Time: 7.09\n",
      "Acc : 89.46\n",
      "\n",
      "Epoch: 15\n",
      "---------------------\n",
      "Epoch [15/50], Loss: 0.110984, Supervised Loss: 0.114687, Unsupervised Loss: 0.031945, Time: 7.01\n",
      "Acc : 88.59\n",
      "\n",
      "Epoch: 16\n",
      "---------------------\n",
      "Epoch [16/50], Loss: 0.104523, Supervised Loss: 0.107995, Unsupervised Loss: 0.031047, Time: 7.11\n",
      "Acc : 88.74\n",
      "\n",
      "Epoch: 17\n",
      "---------------------\n",
      "Epoch [17/50], Loss: 0.098596, Supervised Loss: 0.101863, Unsupervised Loss: 0.030393, Time: 7.19\n",
      "Acc : 88.58\n",
      "\n",
      "Epoch: 18\n",
      "---------------------\n",
      "Epoch [18/50], Loss: 0.094234, Supervised Loss: 0.097320, Unsupervised Loss: 0.029739, Time: 7.20\n",
      "Acc : 88.93\n",
      "\n",
      "Epoch: 19\n",
      "---------------------\n",
      "Epoch [19/50], Loss: 0.089495, Supervised Loss: 0.092418, Unsupervised Loss: 0.029011, Time: 7.25\n",
      "Acc : 91.38\n",
      "\n",
      "Epoch: 20\n",
      "---------------------\n",
      "Epoch [20/50], Loss: 0.085042, Supervised Loss: 0.087820, Unsupervised Loss: 0.028197, Time: 7.16\n",
      "Acc : 90.57\n",
      "\n",
      "Epoch: 21\n",
      "---------------------\n",
      "Epoch [21/50], Loss: 0.081001, Supervised Loss: 0.083645, Unsupervised Loss: 0.027509, Time: 7.21\n",
      "Acc : 90.79\n",
      "\n",
      "Epoch: 22\n",
      "---------------------\n",
      "Epoch [22/50], Loss: 0.077878, Supervised Loss: 0.080402, Unsupervised Loss: 0.026744, Time: 7.11\n",
      "Acc : 92.01\n",
      "\n",
      "Epoch: 23\n",
      "---------------------\n",
      "Epoch [23/50], Loss: 0.074685, Supervised Loss: 0.077099, Unsupervised Loss: 0.026108, Time: 7.27\n",
      "Acc : 92.31\n",
      "\n",
      "Epoch: 24\n",
      "---------------------\n",
      "Epoch [24/50], Loss: 0.071573, Supervised Loss: 0.073887, Unsupervised Loss: 0.025461, Time: 7.22\n",
      "Acc : 93.23\n",
      "\n",
      "Epoch: 25\n",
      "---------------------\n",
      "Epoch [25/50], Loss: 0.069076, Supervised Loss: 0.071298, Unsupervised Loss: 0.024860, Time: 7.13\n",
      "Acc : 93.41\n",
      "\n",
      "Epoch: 26\n",
      "---------------------\n",
      "Epoch [26/50], Loss: 0.066421, Supervised Loss: 0.068557, Unsupervised Loss: 0.024256, Time: 6.85\n",
      "Acc : 94.00\n",
      "\n",
      "Epoch: 27\n",
      "---------------------\n",
      "Epoch [27/50], Loss: 0.064366, Supervised Loss: 0.066422, Unsupervised Loss: 0.023687, Time: 7.07\n",
      "Acc : 93.84\n",
      "\n",
      "Epoch: 28\n",
      "---------------------\n",
      "Epoch [28/50], Loss: 0.062067, Supervised Loss: 0.064050, Unsupervised Loss: 0.023137, Time: 7.27\n",
      "Acc : 94.53\n",
      "\n",
      "Epoch: 29\n",
      "---------------------\n",
      "Epoch [29/50], Loss: 0.060003, Supervised Loss: 0.061918, Unsupervised Loss: 0.022621, Time: 7.28\n",
      "Acc : 94.68\n",
      "\n",
      "Epoch: 30\n",
      "---------------------\n",
      "Epoch [30/50], Loss: 0.058016, Supervised Loss: 0.059867, Unsupervised Loss: 0.022154, Time: 7.16\n",
      "Acc : 94.65\n",
      "\n",
      "Epoch: 31\n",
      "---------------------\n",
      "Epoch [31/50], Loss: 0.056547, Supervised Loss: 0.058338, Unsupervised Loss: 0.021700, Time: 6.98\n",
      "Acc : 94.71\n",
      "\n",
      "Epoch: 32\n",
      "---------------------\n",
      "Epoch [32/50], Loss: 0.054874, Supervised Loss: 0.056609, Unsupervised Loss: 0.021272, Time: 6.89\n",
      "Acc : 95.17\n",
      "\n",
      "Epoch: 33\n",
      "---------------------\n",
      "Epoch [33/50], Loss: 0.053404, Supervised Loss: 0.055086, Unsupervised Loss: 0.020874, Time: 6.94\n",
      "Acc : 95.11\n",
      "\n",
      "Epoch: 34\n",
      "---------------------\n",
      "Epoch [34/50], Loss: 0.052079, Supervised Loss: 0.053712, Unsupervised Loss: 0.020485, Time: 7.18\n",
      "Acc : 94.88\n",
      "\n",
      "Epoch: 35\n",
      "---------------------\n",
      "Epoch [35/50], Loss: 0.050591, Supervised Loss: 0.052177, Unsupervised Loss: 0.020104, Time: 7.18\n",
      "Acc : 95.33\n",
      "\n",
      "Epoch: 36\n",
      "---------------------\n",
      "Epoch [36/50], Loss: 0.049292, Supervised Loss: 0.050834, Unsupervised Loss: 0.019764, Time: 7.23\n",
      "Acc : 95.36\n",
      "\n",
      "Epoch: 37\n",
      "---------------------\n",
      "Epoch [37/50], Loss: 0.049008, Supervised Loss: 0.050508, Unsupervised Loss: 0.019499, Time: 7.06\n",
      "Acc : 95.55\n",
      "\n",
      "Epoch: 38\n",
      "---------------------\n",
      "Epoch [38/50], Loss: 0.047746, Supervised Loss: 0.049206, Unsupervised Loss: 0.019205, Time: 6.95\n",
      "Acc : 95.69\n",
      "\n",
      "Epoch: 39\n",
      "---------------------\n",
      "Epoch [39/50], Loss: 0.046994, Supervised Loss: 0.048417, Unsupervised Loss: 0.018916, Time: 7.08\n",
      "Acc : 95.69\n",
      "\n",
      "Epoch: 40\n",
      "---------------------\n",
      "Epoch [40/50], Loss: 0.045828, Supervised Loss: 0.047215, Unsupervised Loss: 0.018623, Time: 7.09\n",
      "Acc : 95.55\n",
      "\n",
      "Epoch: 41\n",
      "---------------------\n",
      "Epoch [41/50], Loss: 0.044710, Supervised Loss: 0.046063, Unsupervised Loss: 0.018343, Time: 7.11\n",
      "Acc : 95.93\n",
      "\n",
      "Epoch: 42\n",
      "---------------------\n",
      "Epoch [42/50], Loss: 0.043645, Supervised Loss: 0.044967, Unsupervised Loss: 0.018057, Time: 6.99\n",
      "Acc : 95.77\n",
      "\n",
      "Epoch: 43\n",
      "---------------------\n",
      "Epoch [43/50], Loss: 0.042761, Supervised Loss: 0.044051, Unsupervised Loss: 0.017791, Time: 7.36\n",
      "Acc : 95.68\n",
      "\n",
      "Epoch: 44\n",
      "---------------------\n",
      "Epoch [44/50], Loss: 0.041811, Supervised Loss: 0.043072, Unsupervised Loss: 0.017555, Time: 7.16\n",
      "Acc : 95.82\n",
      "\n",
      "Epoch: 45\n",
      "---------------------\n",
      "Epoch [45/50], Loss: 0.041326, Supervised Loss: 0.042559, Unsupervised Loss: 0.017360, Time: 7.18\n",
      "Acc : 96.20\n",
      "\n",
      "Epoch: 46\n",
      "---------------------\n",
      "Epoch [46/50], Loss: 0.040429, Supervised Loss: 0.041635, Unsupervised Loss: 0.017130, Time: 7.20\n",
      "Acc : 96.49\n",
      "\n",
      "Epoch: 47\n",
      "---------------------\n",
      "Epoch [47/50], Loss: 0.039798, Supervised Loss: 0.040978, Unsupervised Loss: 0.016894, Time: 7.33\n",
      "Acc : 96.30\n",
      "\n",
      "Epoch: 48\n",
      "---------------------\n",
      "Epoch [48/50], Loss: 0.039002, Supervised Loss: 0.040157, Unsupervised Loss: 0.016659, Time: 6.95\n",
      "Acc : 96.11\n",
      "\n",
      "Epoch: 49\n",
      "---------------------\n",
      "Epoch [49/50], Loss: 0.038262, Supervised Loss: 0.039394, Unsupervised Loss: 0.016447, Time: 6.93\n",
      "Acc : 96.00\n",
      "\n",
      "Epoch: 50\n",
      "---------------------\n",
      "Epoch [50/50], Loss: 0.037506, Supervised Loss: 0.038615, Unsupervised Loss: 0.016240, Time: 6.98\n",
      "Acc : 96.21\n",
      "Acc (best model): 96.21\n",
      "\n",
      "Epoch: 1\n",
      "---------------------\n",
      "Epoch [1/50], Loss: 0.789919, Supervised Loss: 0.830912, Unsupervised Loss: 0.066612, Time: 7.29\n",
      "Acc : 22.03\n",
      "\n",
      "Epoch: 2\n",
      "---------------------\n",
      "Epoch [2/50], Loss: 0.548441, Supervised Loss: 0.573907, Unsupervised Loss: 0.067626, Time: 7.47\n",
      "Acc : 40.86\n",
      "\n",
      "Epoch: 3\n",
      "---------------------\n",
      "Epoch [3/50], Loss: 0.425480, Supervised Loss: 0.444343, Unsupervised Loss: 0.062863, Time: 7.22\n",
      "Acc : 70.66\n",
      "\n",
      "Epoch: 4\n",
      "---------------------\n",
      "Epoch [4/50], Loss: 0.345890, Supervised Loss: 0.360573, Unsupervised Loss: 0.059853, Time: 7.25\n",
      "Acc : 71.76\n",
      "\n",
      "Epoch: 5\n",
      "---------------------\n",
      "Epoch [5/50], Loss: 0.292430, Supervised Loss: 0.304251, Unsupervised Loss: 0.055706, Time: 7.04\n",
      "Acc : 79.71\n",
      "\n",
      "Epoch: 6\n",
      "---------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6d1e4bfd9a4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                                                  k=cfg['k'], n_classes=10, seed=seed, shuffle_train=False)\n\u001b[0;32m     14\u001b[0m     train(model, train_loader, val_loader,seed, cfg['k'],cfg['alpha'],cfg['learning_rate'],\n\u001b[1;32m---> 15\u001b[1;33m          cfg['num_epochs'], cfg['batch_size'], ntrain)\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-96dfa193edf3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, seed, k, alpha, lr, num_epochs, batch_size, ntrain, n_classes, max_epochs, max_val)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# targets change only once per epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[1;31m#print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# retrieve batch size again cause drop last is false\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    867\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_python3.7\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = vars(config())\n",
    "\n",
    "# prepare data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cfg['m'], cfg['s'])])\n",
    "train_dataset, val_dataset = mnist_dataset(root='~/datasets/MNIST', transform=transform)\n",
    "ntrain = len(train_dataset)\n",
    "\n",
    "\n",
    "for i in range(cfg['n_exp']):\n",
    "    model = CNN(cfg['batch_size'], cfg['std'], fm1=cfg['fm1'], fm2=cfg['fm2']).cuda()\n",
    "    seed = cfg['seeds'][i]\n",
    "    train_loader, val_loader, indices = sample_train(train_dataset, val_dataset, batch_size=cfg['batch_size'],\n",
    "                                                 k=cfg['k'], n_classes=10, seed=seed, shuffle_train=False)\n",
    "    train(model, train_loader, val_loader,seed, cfg['k'],cfg['alpha'],cfg['learning_rate'],\n",
    "         cfg['num_epochs'], cfg['batch_size'], ntrain)\n",
    "    evaluation(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
